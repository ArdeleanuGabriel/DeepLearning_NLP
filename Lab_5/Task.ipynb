{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ardeleanu\n",
      "[nltk_data]     Gabriel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "folder_path = \"wikipedia_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wikipedia_content_to_files(topic, num_titles=5):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('MyProjectName (merlin@example.com)', 'en')\n",
    "    search_page = wiki_wiki.page(topic)\n",
    "\n",
    "    linked_pages = search_page.links\n",
    "    linked_titles = list(linked_pages.keys())\n",
    "\n",
    "    random_titles = random.sample(linked_titles, num_titles)\n",
    "\n",
    "    directory = \"wikipedia_content\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    result = []\n",
    "    for title in random_titles:\n",
    "        page = wiki_wiki.page(title)\n",
    "        content = page.text\n",
    "        result.append((title, content))\n",
    "        \n",
    "        filename = os.path.join(directory, f\"{title}.txt\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        print(f\"Saved content to {filename}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_wikipedia_content_to_files(\"movies\")\n",
    "#save_wikipedia_content_to_files(\"drinks\")\n",
    "#save_wikipedia_content_to_files(\"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files(folder_path):\n",
    "    stemmer = PorterStemmer()\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    content = \"\"\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content += file.read()\n",
    "\n",
    "    tokens = content.split()\n",
    "    processed_tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words]\n",
    "    processed_text = \" \".join(processed_tokens)\n",
    "\n",
    "    documents.append(processed_text)\n",
    "\n",
    "    #Bag-of-Words\n",
    "    bow_matrix = count_vectorizer.fit_transform(documents)\n",
    "    bow_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "    #TF-IDF\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Bag-of-Words vector\n",
    "    print(\"\\nBag-of-Words Vector:\")\n",
    "    bow_vector = bow_matrix.toarray()[0]\n",
    "    bowdict = {word: bow_vector[idx] for idx, word in enumerate(bow_feature_names) if bow_vector[idx] > 0}\n",
    "    #print(bow_dict)\n",
    "    with open(\"BOW.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        for key, value in bowdict.items():\n",
    "            #print(f\"key {key}, value {value}\")\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "    # TF-IDF vector\n",
    "    print(\"\\nTF-IDF Vector:\")\n",
    "    tfidf_vector = tfidf_matrix.toarray()[0]\n",
    "    tfidfdict = {word: tfidf_vector[idx] for idx, word in enumerate(tfidf_feature_names) if tfidf_vector[idx] > 0}\n",
    "    #print(tfidfdict)\n",
    "    with open(\"TF_IDF.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        for key, value in tfidfdict.items():\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "    return bowdict, tfidfdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag-of-Words Vector:\n",
      "\n",
      "TF-IDF Vector:\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'wikipedia_content'\n",
    "bow_dict, tfidf_dict = process_text_files(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = list(bow_dict.keys())  # or tfidf_dict.keys()\n",
    "bow_values = np.array(list(bow_dict.values())).reshape(1, -1)  # Single-row matrix for BOW\n",
    "tfidf_values = np.array(list(tfidf_dict.values())).reshape(1, -1)  # Single-row matrix for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    }
   ],
   "source": [
    "# SVD on the single-row BOW vector\n",
    "svd_bow = TruncatedSVD(n_components=3)\n",
    "bow_lsa = svd_bow.fit_transform(bow_values)\n",
    "\n",
    "# SVD on the single-row TF-IDF vector\n",
    "svd_tfidf = TruncatedSVD(n_components=3)\n",
    "tfidf_lsa = svd_tfidf.fit_transform(tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA result on BOW (single vector, 3 concepts):\n",
      "[[1062.42082058]]\n",
      "\n",
      "LSA result on TF-IDF (single vector, 3 concepts):\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"LSA result on BOW (single vector, 3 concepts):\")\n",
    "print(bow_lsa)\n",
    "\n",
    "print(\"\\nLSA result on TF-IDF (single vector, 3 concepts):\")\n",
    "print(tfidf_lsa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'wikipedia_content'\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        documents.append(preprocess_text(text))\n",
    "\n",
    "num_topics = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW Vectorization\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA result on BOW (3 topics):\n",
      "[[ 1.00305373e+02  3.04051533e+02 -1.33107980e+02]\n",
      " [ 8.49288228e+01  1.43093802e+02  2.28747134e+02]\n",
      " [ 4.35814738e+02 -1.13501256e+02 -2.48573969e+01]\n",
      " [ 6.27357934e+00  8.25661747e+00  5.47861804e+00]\n",
      " [ 4.55903364e+01  4.17823726e+01  4.33690014e+01]\n",
      " [ 6.27016450e-01  4.14317495e-01  4.63677946e-01]\n",
      " [ 4.31632438e+01  3.68391819e+01  2.75433570e+01]\n",
      " [ 5.27078452e+00  5.23873518e+00  2.51240858e+00]\n",
      " [ 3.66028304e+00  1.92828088e+00  1.64052058e+00]\n",
      " [ 4.47843278e-01  2.76801087e-01  3.26206715e-01]\n",
      " [ 1.87592673e+01  1.89417629e+01  8.96718045e+00]\n",
      " [ 3.29019660e+01  3.01191328e+01  1.60391838e+01]\n",
      " [ 3.30866714e+01  5.54171412e+01  2.43024386e+01]\n",
      " [ 6.67643210e+00  5.22815271e+00  3.72729272e+00]\n",
      " [ 4.67968632e+00  3.87196476e+00  2.65146569e+00]]\n",
      "\n",
      "Normalized LSA result on BOW (unit norm):\n",
      "[[ 0.28928411  0.87689495 -0.38388794]\n",
      " [ 0.30024248  0.50586875  0.80867253]\n",
      " [ 0.96624925 -0.25164478 -0.05511159]\n",
      " [ 0.53492554  0.70401206  0.46714205]\n",
      " [ 0.60358796  0.55317287  0.57417885]\n",
      " [ 0.7100447   0.46918058  0.52507724]\n",
      " [ 0.68428422  0.58402633  0.43665589]\n",
      " [ 0.67189992  0.66781439  0.32027246]\n",
      " [ 0.82243695  0.43326962  0.36861213]\n",
      " [ 0.72308703  0.44692259  0.52669284]\n",
      " [ 0.66695604  0.67344438  0.3188139 ]\n",
      " [ 0.69410388  0.63539689  0.33836457]\n",
      " [ 0.479749    0.8035356   0.35237968]\n",
      " [ 0.72077243  0.56441948  0.40239005]\n",
      " [ 0.70611665  0.58423976  0.40007897]]\n",
      "\n",
      "LSA result on TF-IDF (3 topics):\n",
      "[[ 0.35716645  0.0263025   0.50279488]\n",
      " [ 0.36493237  0.03503019  0.43911182]\n",
      " [ 0.25289463 -0.01180229  0.03560287]\n",
      " [ 0.22899923  0.01025383  0.33487695]\n",
      " [ 0.61517501 -0.35212443 -0.23797291]\n",
      " [ 0.29125876 -0.27876852 -0.30971017]\n",
      " [ 0.61394037 -0.3386005  -0.28212197]\n",
      " [ 0.16409365  0.04476731  0.24156606]\n",
      " [ 0.24036742  0.4313735  -0.1149473 ]\n",
      " [ 0.24488212 -0.14517406 -0.18450897]\n",
      " [ 0.27092578  0.15266547  0.19558498]\n",
      " [ 0.25334937  0.07333308  0.18426785]\n",
      " [ 0.23116886 -0.00140008  0.2740028 ]\n",
      " [ 0.29520642  0.6408897  -0.23456598]\n",
      " [ 0.26181439  0.61057147 -0.29145334]]\n"
     ]
    }
   ],
   "source": [
    "lsa_bow = TruncatedSVD(num_topics).fit_transform(bow_matrix)\n",
    "lsa_tfidf = TruncatedSVD(num_topics).fit_transform(tfidf_matrix)\n",
    "\n",
    "print(\"LSA result on BOW (3 topics):\")\n",
    "print(lsa_bow)\n",
    "\n",
    "#normalized version\n",
    "lsa_bow_normalized = normalize(lsa_bow, norm='l2', axis=1)\n",
    "print()\n",
    "print(\"Normalized LSA result on BOW (unit norm):\")\n",
    "print(lsa_bow_normalized)\n",
    "\n",
    "print(\"\\nLSA result on TF-IDF (3 topics):\")\n",
    "print(lsa_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Topic Matrix (W):\n",
      "[[0.         0.         0.63759549]\n",
      " [0.         0.         0.6173946 ]\n",
      " [0.10844715 0.02265308 0.18100322]\n",
      " [0.         0.         0.42835028]\n",
      " [0.57239398 0.         0.01825651]\n",
      " [0.3451726  0.         0.        ]\n",
      " [0.5711115  0.00671449 0.00743697]\n",
      " [0.         0.         0.29572751]\n",
      " [0.00765448 0.53042129 0.00771921]\n",
      " [0.25013341 0.00410106 0.        ]\n",
      " [0.         0.09337505 0.36784966]\n",
      " [0.01111176 0.03007721 0.34233233]\n",
      " [0.00369974 0.         0.38181126]\n",
      " [0.         0.7750044  0.        ]\n",
      " [0.         0.73578363 0.        ]]\n",
      "\n",
      "\n",
      "Topic-Term Matrix (H):\n",
      "[[3.16045214e-04 6.32090428e-04 3.16045214e-04 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.70825930e-05 5.41651860e-05 2.70825930e-05 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.04197636e-04 6.08395272e-04 3.04197636e-04 ... 1.81484844e-03\n",
      "  1.81484844e-03 4.51497306e-03]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "nmf_model.fit(tfidf_matrix)\n",
    "\n",
    "#topic-term matrix (num_topics, num_terms)\n",
    "topic_term_matrix = nmf_model.components_\n",
    "\n",
    "#document-topic matrix (num_documents, num_topics)\n",
    "doc_topic_matrix = nmf_model.transform(tfidf_matrix)\n",
    "\n",
    "print(\"Document-Topic Matrix (W):\")\n",
    "print(doc_topic_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Topic-Term Matrix (H):\")\n",
    "print(topic_term_matrix)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "Top terms: film documentari dunkirk nolan best\n",
      "Topic #1:\n",
      "Top terms: vehicl wheelbas tyre brake displaystyl\n",
      "Topic #2:\n",
      "Top terms: archaeolog africa african culinari seder\n"
     ]
    }
   ],
   "source": [
    "#feature names\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "top_topics = 5\n",
    "\n",
    "for topic_idx, topic in enumerate(topic_term_matrix):\n",
    "    top_terms_idx = topic.argsort()[-top_topics:][::-1] #topics - top n = top_topics words, descending order\n",
    "    top_terms = [terms[i] for i in top_terms_idx]\n",
    "    print(f\"Topic #{topic_idx}:\")\n",
    "    print(\"Top terms:\", \" \".join(top_terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #1 is most strongly associated with Topic #3 - weight of 0.6376\n",
      "Document #2 is most strongly associated with Topic #3 - weight of 0.6174\n",
      "Document #3 is most strongly associated with Topic #3 - weight of 0.1810\n",
      "Document #4 is most strongly associated with Topic #3 - weight of 0.4284\n",
      "Document #5 is most strongly associated with Topic #1 - weight of 0.5724\n",
      "Document #6 is most strongly associated with Topic #1 - weight of 0.3452\n",
      "Document #7 is most strongly associated with Topic #1 - weight of 0.5711\n",
      "Document #8 is most strongly associated with Topic #3 - weight of 0.2957\n",
      "Document #9 is most strongly associated with Topic #2 - weight of 0.5304\n",
      "Document #10 is most strongly associated with Topic #1 - weight of 0.2501\n",
      "Document #11 is most strongly associated with Topic #3 - weight of 0.3678\n",
      "Document #12 is most strongly associated with Topic #3 - weight of 0.3423\n",
      "Document #13 is most strongly associated with Topic #3 - weight of 0.3818\n",
      "Document #14 is most strongly associated with Topic #2 - weight of 0.7750\n",
      "Document #15 is most strongly associated with Topic #2 - weight of 0.7358\n"
     ]
    }
   ],
   "source": [
    "for doc_idx, doc in enumerate(doc_topic_matrix):\n",
    "    #highest value topic\n",
    "    top_topic_idx = doc.argmax()\n",
    "    print(f\"Document #{doc_idx + 1} is most strongly associated with Topic #{top_topic_idx + 1} - weight of {doc[top_topic_idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "Top terms: film documentari ekiben use tyre\n",
      "\n",
      "\n",
      "Topic #2:\n",
      "Top terms: disc bluray archaeolog use film\n",
      "\n",
      "\n",
      "Topic #3:\n",
      "Top terms: africa seder african peopl egypt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(num_topics, random_state=42)\n",
    "lda.fit(bow_matrix)\n",
    "\n",
    "#feature names\n",
    "terms = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "top_topics = 5\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_terms_idx = topic.argsort()[-top_topics:][::-1]\n",
    "    top_terms = [terms[i] for i in top_terms_idx]\n",
    "    \n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\"Top terms:\", \" \".join(top_terms))\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
