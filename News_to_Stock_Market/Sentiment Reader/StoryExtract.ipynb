{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = 'train-00000-of-00039.parquet'\n",
    "\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( df.loc[df['id'] == 1, 'descendants'].any() > 0):\n",
    "    print( df.loc[df['id'] == 1, 'descendants'] > 0)\n",
    "    x = df.loc[df['id'] == 1, 'descendants']\n",
    "    print( x + 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['html'] = 'null'  \n",
    "df['comments'] = 'null'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_to_text(parent_id):\n",
    "    visited_ids = set()\n",
    "\n",
    "    def traverse_and_collect(parent_id):\n",
    "        parent = df[df['id'] == parent_id]\n",
    "        if parent.empty:\n",
    "            return #\"Parent not found\"\n",
    "        \n",
    "        visited_ids.add(parent_id)  # add parent to visited ids\n",
    "\n",
    "        kids = parent['kids'].values[0]\n",
    "        if kids is not None:\n",
    "            if not all(kid in df['id'].values for kid in kids):  \n",
    "                return #\"We can't create the story\" if any kid is missing\n",
    "            \n",
    "            current_text = parent['text'].values[0] if parent['text'].values[0] is not None else \"\"\n",
    "            for kid_id in kids:\n",
    "                child_text = traverse_and_collect(kid_id)\n",
    "                if child_text == \"We can't create the story\":\n",
    "                    return #\"We can't create the story\" abort if any child's subtree is incomplete\n",
    "                current_text += \"\\n\" + child_text\n",
    "        else:\n",
    "            current_text = parent['text'].values[0] if parent['text'].values[0] is not None else \"\"\n",
    "\n",
    "        return current_text\n",
    "\n",
    "    final_text = traverse_and_collect(parent_id)\n",
    "    if final_text != \"We can't create the story\" and final_text != \"Parent not found\":\n",
    "        # remove all visited ids except the root parent\n",
    "        global df\n",
    "        df = df[~df['id'].isin(visited_ids - {parent_id})]\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df[\"id\"]:\n",
    "    if df.loc[df['id'] == ind, 'kids'].notnull().any():\n",
    "        df.loc[df['id'] == ind, 'comments'] = parent_to_text(ind)\n",
    "    \n",
    "    if df.loc[df['id'] == ind, 'url'].notnull().any():\n",
    "        url = df.loc[df['id'] == ind, 'url'].values[0]\n",
    "        try:\n",
    "\n",
    "            response = requests.get(url, timeout=5)\n",
    "            response.raise_for_status()  # raise HTTPError for bad responses\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            plain_text = soup.get_text()\n",
    "            \n",
    "            df.loc[df['id'] == ind, 'html'] = plain_text\n",
    "        except (requests.RequestException, Exception) as e:\n",
    "            #print(f\"Error fetching content for URL {url}: {e}\")\n",
    "            df = df[df['id'] != ind]\n",
    "    else:\n",
    "        # ff there is no URL and it hasn't been removed previously remove it\n",
    "        df = df[df['id'] != ind]\n",
    "\n",
    "df.to_parquet('processed_0.parquet', engine='pyarrow', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mind\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ind' is not defined"
     ]
    }
   ],
   "source": [
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('processed_01.parquet', engine='pyarrow', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = []\n",
    "desc = []\n",
    "\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if (df['descendants'].iloc[i] > 0):\n",
    "        parents.append(i)\n",
    "        desc.append(df['descendants'].iloc[i])\n",
    "\n",
    "print(\"Parents:\", parents)\n",
    "print(\"Descendants:\", desc)\n",
    "print(len(parents))\n",
    "print(len(desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "for i in range(df.shape[0]):\n",
    "    if (pd.isna(df.at[i, 'parent']) or pd.isna(df.at[i, 'url'])):\n",
    "        news.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://www.paulgraham.com/startuplessons.html\" \n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # raise an error\n",
    "    my_html = response.text  # store HTML content\n",
    "    print(my_html)  \n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error fetching the URL: {e}\")\n",
    "    my_html = None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Column1': [1, 2, 3], 'Column2': ['A', 'B', 'C']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#df.to_parquet('example.parquet', engine='pyarrow', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_to_text(parent_id):\n",
    "    #visited_ids = set()\n",
    "\n",
    "    def traverse_and_collect(parent_id):\n",
    "        print(f\"Parent: {parent_id}\")\n",
    "        parent = df[df['id'] == parent_id]\n",
    "        if parent.empty:\n",
    "            return #\"Parent not found\"\n",
    "        \n",
    "        #visited_ids.add(parent_id)  # add parent to visited ids\n",
    "\n",
    "        kids = parent['kids'].values[0]\n",
    "        if kids is not None:\n",
    "            if not all(kid in df['id'].values for kid in kids):  \n",
    "                return #\"We can't create the story\" if any kid is missing\n",
    "            \n",
    "            current_text = parent['text'].values[0] if parent['text'].values[0] is not None else \"\"\n",
    "            for kid_id in kids:\n",
    "                child_text = traverse_and_collect(kid_id)\n",
    "                if child_text == \"We can't create the story\":\n",
    "                    return #\"We can't create the story\" abort if any child's subtree is incomplete\n",
    "                current_text += \". \" + child_text\n",
    "        else:\n",
    "            current_text = parent['text'].values[0] if parent['text'].values[0] is not None else \"\"\n",
    "\n",
    "        return current_text\n",
    "\n",
    "    final_text = traverse_and_collect(parent_id)\n",
    "    #if final_text != \"We can't create the story\" and final_text != \"Parent not found\":\n",
    "        # remove all visited ids except the root parent\n",
    "        #global df\n",
    "        #df = df[~df['id'].isin(visited_ids - {parent_id})]\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df[\"id\"]:\n",
    "    if df.loc[df['id'] == ind, 'descendants'] > 0:\n",
    "        df.loc[df['id'] == ind, 'comments'] = parent_to_text(ind)\n",
    "    \n",
    "    if df.loc[df['id'] == ind, 'url'].notnull().any():\n",
    "        url = df.loc[df['id'] == ind, 'url'].values[0]\n",
    "        try:\n",
    "\n",
    "            response = requests.get(url, timeout=5)\n",
    "            response.raise_for_status()  # raise HTTPError for bad responses\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            plain_text = soup.get_text()\n",
    "            \n",
    "            df.loc[df['id'] == ind, 'html'] = plain_text\n",
    "        except (requests.RequestException, Exception) as e:\n",
    "            #print(f\"Error fetching content for URL {url}: {e}\")\n",
    "            df = df[df['id'] != ind]\n",
    "    else:\n",
    "        # ff there is no URL and it hasn't been removed previously remove it\n",
    "        df = df[df['id'] != ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('processed_0.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Threadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Lock to ensure thread-safe DataFrame modifications\n",
    "df_lock = threading.Lock()\n",
    "\n",
    "def fetch_url_and_process(ind):\n",
    "    global df\n",
    "    with df_lock:\n",
    "        url = df.loc[df['id'] == ind, 'url'].values[0]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        plain_text = soup.get_text()\n",
    "\n",
    "        # Thread-safe update of the DataFrame\n",
    "        with df_lock:\n",
    "            df.loc[df['id'] == ind, 'html'] = plain_text\n",
    "    except (requests.RequestException, Exception) as e:\n",
    "        # Thread-safe removal of the row\n",
    "        with df_lock:\n",
    "            df = df[df['id'] != ind]\n",
    "\n",
    "def process_descendants(ind):\n",
    "    global df\n",
    "    with df_lock:\n",
    "        #print( df.loc[df['id'] == ind, 'descendants'])\n",
    "        if df.loc[df['id'] == ind, 'descendants'].any() > 0: # JOKES ON YOU DE AICI NU MAI FAC NIMIC\n",
    "            df.loc[df['id'] == ind, 'comments'] = parent_to_text(ind) # NIMIC SCRIS IN COMMENTS\n",
    "\n",
    "# Main function to manage threading\n",
    "def process_dataframe(df):\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        futures = []\n",
    "        for ind in df[\"id\"]:\n",
    "            # Submit tasks for descendant processing\n",
    "            futures.append(executor.submit(process_descendants, ind))\n",
    "\n",
    "            # Submit tasks for URL fetching\n",
    "            if df.loc[df['id'] == ind, 'url'].notnull().any():\n",
    "                futures.append(executor.submit(fetch_url_and_process, ind))\n",
    "            else:\n",
    "                # Thread-safe removal for rows without a URL\n",
    "                with df_lock:\n",
    "                    df = df[df['id'] != ind]\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Retrieve results (if any) and raise exceptions\n",
    "            except Exception as e:\n",
    "                print(f\"Error in thread: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[\"type\", \"descendants\" , \"comments\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('processed_0_threadding.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import threading\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "file_path = 'train-00000-of-00039.parquet'\n",
    "\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['html'] = 'null'  \n",
    "df['comments'] = 'null'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_to_text(parent_id):\n",
    "    #visited_ids = set()\n",
    "\n",
    "    def traverse_and_collect(parent_id):\n",
    "        parent = df[df['id'] == parent_id]\n",
    "        if parent.empty:\n",
    "            return #\"Parent not found\"\n",
    "        \n",
    "        #visited_ids.add(parent_id)  # add parent to visited ids\n",
    "\n",
    "        kids = parent['kids'].values[0]\n",
    "        if kids is not None:\n",
    "            if not all(kid in df['id'].values for kid in kids):  \n",
    "                return #\"We can't create the story\" if any kid is missing\n",
    "            \n",
    "            current_text = parent['text'].values[0] if parent['text'].values[0] is not None else \"\"\n",
    "            for kid_id in kids:\n",
    "                child_text = traverse_and_collect(kid_id)\n",
    "                if child_text == \"We can't create the story\":\n",
    "                    return #\"We can't create the story\" abort if any child's subtree is incomplete\n",
    "                current_text += \". \" + child_text\n",
    "        else:\n",
    "            current_text = parent['text'].values[0] if parent['text'].values[0] is not None else \"\"\n",
    "\n",
    "        return current_text\n",
    "\n",
    "    final_text = traverse_and_collect(parent_id)\n",
    "    #if final_text != \"We can't create the story\" and final_text != \"Parent not found\":\n",
    "        # remove all visited ids except the root parent\n",
    "        #global df\n",
    "        #df = df[~df['id'].isin(visited_ids - {parent_id})]\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lock to ensure thread-safe DataFrame modifications\n",
    "df_lock = threading.Lock()\n",
    "\n",
    "def fetch_url_and_process(ind):\n",
    "    global df\n",
    "    with df_lock:\n",
    "        url = df.loc[df['id'] == ind, 'url'].values[0]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        plain_text = soup.get_text()\n",
    "\n",
    "        # Thread-safe update of the DataFrame\n",
    "        with df_lock:\n",
    "            df.loc[df['id'] == ind, 'html'] = plain_text\n",
    "    except (requests.RequestException, Exception) as e:\n",
    "        # Thread-safe removal of the row\n",
    "        with df_lock:\n",
    "            df = df[df['id'] != ind]\n",
    "\n",
    "def process_descendants(ind):\n",
    "    global df\n",
    "    with df_lock:\n",
    "        #print( df.loc[df['id'] == ind, 'descendants'])\n",
    "        if df.loc[df['id'] == ind, 'descendants'].any() > 0: # JOKES ON YOU DE AICI NU MAI FAC NIMIC\n",
    "            df.loc[df['id'] == ind, 'comments'] = parent_to_text(ind) # NIMIC SCRIS IN COMMENTS\n",
    "\n",
    "# Main function to manage threading\n",
    "def process_dataframe(df):\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        futures = []\n",
    "        for ind in df[\"id\"]:\n",
    "            # Submit tasks for descendant processing\n",
    "            futures.append(executor.submit(process_descendants, ind))\n",
    "\n",
    "            # Submit tasks for URL fetching\n",
    "            if df.loc[df['id'] == ind, 'url'].notnull().any():\n",
    "                futures.append(executor.submit(fetch_url_and_process, ind))\n",
    "            else:\n",
    "                # Thread-safe removal for rows without a URL\n",
    "                with df_lock:\n",
    "                    df = df[df['id'] != ind]\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Retrieve results (if any) and raise exceptions\n",
    "            except Exception as e:\n",
    "                print(f\"Error in thread: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n",
      "C:\\Users\\Ardeleanu Gabriel\\AppData\\Local\\Temp\\ipykernel_6276\\2273599182.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[95]\u001b[0m, in \u001b[0;36mprocess_dataframe\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('processed_0_threadding_2054_35.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   type          by                time  \\\n",
      "0   0   None        None                 NaT   \n",
      "1   1  story          pg 2006-10-09 18:21:51   \n",
      "2   2  story     phyllis 2006-10-09 18:30:28   \n",
      "5   5  story      perler 2006-10-09 18:51:04   \n",
      "8   8  story  frobnicate 2006-10-09 19:17:39   \n",
      "\n",
      "                                               title  text  \\\n",
      "0                                               None  None   \n",
      "1                                       Y Combinator  None   \n",
      "2                      A Student's Guide to Startups  None   \n",
      "5  Google, YouTube acquisition announcement could...  None   \n",
      "8                         LikeBetter featured by BBC  None   \n",
      "\n",
      "                                                 url  score  parent  \\\n",
      "0                                               None    NaN     NaN   \n",
      "1                             http://ycombinator.com   57.0     NaN   \n",
      "2                 http://www.paulgraham.com/mit.html   16.0     NaN   \n",
      "5  http://www.techcrunch.com/2006/10/09/google-yo...    7.0     NaN   \n",
      "8  http://news.bbc.co.uk/2/hi/programmes/click_on...   10.0     NaN   \n",
      "\n",
      "   top_level_parent  descendants                         kids deleted  dead  \\\n",
      "0                 0          NaN                         None    True  None   \n",
      "1                 1         15.0  [15, 234509, 487171, 82729]    None  None   \n",
      "2                 2          0.0                         None    None  None   \n",
      "5                 5          0.0                         None    None  None   \n",
      "8                 8          0.0                         None    None  None   \n",
      "\n",
      "                                                html  \\\n",
      "0                                               null   \n",
      "1  Y Combinator\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
      "2  A Student's Guide to Startups\\n\\n\\n\\nWant to s...   \n",
      "5  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGoogle, YouTube sign mor...   \n",
      "8  \\n\\n\\nBBC NEWS | Programmes | Click | Webscape...   \n",
      "\n",
      "                                            comments  \n",
      "0                                               null  \n",
      "1  . &#34;the rising star of venture capital&#34;...  \n",
      "2                                               null  \n",
      "5                                               null  \n",
      "8                                               null  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
